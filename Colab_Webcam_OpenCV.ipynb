{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 如何在Colab上使用Webcam并运行OpenCV自定义影像处理函式\n",
        "\n",
        "作者： Jack OmniXRI, 2022/09/07\n",
        "\n",
        "本范列程式共提供两种模式：  \n",
        "\n",
        "**模式一：**使用网路摄影机连续取像并显示，按下取像键后才使用OpenCV进行自定义影像处理及显示结果。\n",
        "\n",
        "**模式二：**使用网路摄影机连续取像并显示，在取像过程同时执行OpenCV自定义影像处理及显示结果。"
      ],
      "metadata": {
        "id": "9JV4JSdbGnrn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lWeSCrmTE57B"
      },
      "outputs": [],
      "source": [
        "# 導入工作必要函式庫\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 转换JavaScript影像物件变成OpenCV格式影像函式\n",
        "\n",
        "输入参数：  \n",
        "js_reply : JavaScript物件，内含从网路摄影像取得之影像  \n",
        "\n",
        "回返结果:  \n",
        "img : OpenCV BGR格式之影像  "
      ],
      "metadata": {
        "id": "qUb1eSkYMNWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def js_to_image(js_reply):\n",
        "  # 解碼成 base64 格式影像\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # 轉換影像變成 Numpy 格式\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # 解碼 Numpy格式到 OpenCV BGR 影像格式\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img"
      ],
      "metadata": {
        "id": "nLxYK8ccMMI-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 自定義OpenCV影像處理函式\n",
        "\n",
        "可自行加入任意處理OpenCV函式，這裡僅使用一個很簡單的影像色彩反轉為例。  \n",
        "註：JavaScript預設取得及顯示彩色影像格式為RGB，若使用BGR格式就會產生色彩反轉。  \n",
        "\n",
        "輸入參數：  \n",
        "img : OpenCV BGR 格式之輸入影像\n",
        "\n",
        "回返結果:  \n",
        "bgr : OpenCV BGR 格式之輸出影像  \n"
      ],
      "metadata": {
        "id": "nynzo8OaSUxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def img_process(img):\n",
        "\n",
        "    bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    return bgr"
      ],
      "metadata": {
        "id": "leMEPsnYSXHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Colab 網路攝影機取像函式\n",
        "\n",
        "本函式由Colab左側選單中「程式碼片段」新增「Camera Capture」而得。  \n",
        "\n",
        "預設呼叫後會產生一個「Capture」按鍵和一個視窗顯示網路攝影機(Webcam)目前取得的連續影像。  \n",
        "\n",
        "按下「Capture」鍵取得影像後，可使用OpenCV來進行任意影像處理及繪製線、框、文字到影像上，再顯示結果。    "
      ],
      "metadata": {
        "id": "V8pPpB-iGmtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from IPython.display import display, Javascript\n",
        "#from google.colab.output import eval_js\n",
        "#from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # 取得JavaScript產生的影像物件\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "\n",
        "  # 取得影像並加入自定義之OpenCV影像處理程式\n",
        "  img = js_to_image(data)\n",
        "  result_img = img_process(img)\n",
        "\n",
        "  ## 將 JavaScript 取得的影像解碼成 base64 格式\n",
        "  #binary = b64decode(data.split(',')[1])\n",
        "\n",
        "  ## 將檔案寫入名為 filename 的檔案中\n",
        "  #with open(filename, 'wb') as f:\n",
        "  #  f.write(binary)\n",
        "  #return filename\n",
        "\n",
        "  # 可改用 OpenCV imwrite() 函式取代原有檔案寫入動作，可省去轉換 base64 格式動作\n",
        "  cv2.imwrite(\"original.jpg\", img) #\n",
        "  cv2.imwrite(filename, result_img)\n",
        "  return filename"
      ],
      "metadata": {
        "id": "YY6a9QzmE_hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 等待網路攝影機取像後進行影像處理及顯示結果\n",
        "\n",
        "\n",
        "按下「Capture」會結束程式並將最後一張影像存入檔案「photo.jpg」中，此為預設檔名亦可於呼叫時另行指定。  \n",
        "\n",
        "同時會執行自定義影像處理函式，並顯示原始及處理後影像內容。  "
      ],
      "metadata": {
        "id": "4nBIinh4xXeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from IPython.display import Image\n",
        "try:\n",
        "  # 啟動網路攝影機連續取像，按下「Capture」後，取得靜態影像，經自定義影像處理後並顯示，預設存檔檔名 photo.jpg\n",
        "  result_filename = take_photo()\n",
        "\n",
        "  # 顯示網路攝影機取得之原始影像\n",
        "  original_filename = 'original.jpg'\n",
        "  print('Original Image : {}'.format(original_filename))\n",
        "  display(Image(original_filename))\n",
        "\n",
        "  # 顯示影像處理後之結果影像\n",
        "  print('Processed Image : {}'.format(result_filename))\n",
        "  display(Image(result_filename))\n",
        "\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "metadata": {
        "id": "8RPb-5hqE_hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**透過以上程式會發現，連續取像過程並不會執行自定義影像處理，只有最後按下「Capture」鍵後才會執行及顯示自定義影像處理函式，所以上述作法只適合靜態結果輸出。**  \n",
        "\n",
        "接著說明如何在網路攝影機取像過程中就直接執行自定義影像處理的方式。  \n",
        "\n",
        "由於Colab是雲端服務，所以本地端網路攝影機取得的影像要先上傳，再透過網頁元件顯示獲取的影像，收到後才能再提供給其它程式（如OpenCV, PIL...)進行處理分析。而計算所得的結果（如線、框、文字）需要畫在一個具有透明圖層（Alpha Channel)的影像上，即產生一張去背影像，再套疊到下一個擷取到的影格上，而非目前的影格，所以顯示出來的結果會有一個影格的延遲。若遇到被偵測的物件移動較快時，顯示上會有跟不上的情況產生，是正常的。  \n",
        "\n"
      ],
      "metadata": {
        "id": "DuhmpmUKqNLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 从网络摄像头获取及显示连续视频流相关函数：\n",
        "\n",
        "video_stream()：设置视频流显示环境（包括视频流显示及上下文字说明区），并启动网络摄像头连续取像显示。\n",
        "removeDom()：移除网页文件对象模型。\n",
        "onAnimationFrame()：将叠加影像绘制到帧上。\n",
        "createDom()：建立网页文件对象模型。\n",
        "stream_frame()：获取流帧内容。\n",
        "video_frame()：获取当前帧内容。\n",
        "video_frame() 取得目前影格內容  \n",
        "\n",
        "參考資料來源： https://colab.research.google.com/drive/1QnC7lV7oVFk5OZCm75fqbLAfD9qBy9bw?usp=sharing"
      ],
      "metadata": {
        "id": "1puuuE3x6Hno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "PRwOslZpynKb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 自定义即时影像处理函式\n",
        "\n",
        "可任意加入自定义OpenCV, PIL影像处理函式，这里以随机绘制一个方块（空心红框或实心黄方块）作为范例。\n",
        "\n",
        "输入参数：  \n",
        "img : OpenCV BGR 格式之输入影像\n",
        "\n",
        "回返结果:  \n",
        "bgra : OpenCV BGRA 格式之输出影像"
      ],
      "metadata": {
        "id": "9ZKlXbViGbYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def realtime_process(img):\n",
        "   # 產生一個具透明圖層的影像，即 OpenCV BGRA 格式。\n",
        "   # [:,:,3]即為透明層設定，初始值為0，即為全透明。\n",
        "   bgra = np.zeros([img.shape[0],img.shape[1],4], dtype=np.uint8)\n",
        "\n",
        "   # 產生一組隨機值\n",
        "   sx = random.randrange(100,400,10) # 起始X座標\n",
        "   sy = random.randrange(100,300,10) # 起始Y座標\n",
        "   sw = random.randrange(50,200,10) # 方塊寬度\n",
        "   sh = random.randrange(50,200,10) # 方塊高度\n",
        "   sc = random.randrange(0,2,1) # 方塊型式及色彩\n",
        "\n",
        "   if(sc == 0):\n",
        "       # 隨意在影像上繪製一個紅色空心方框\n",
        "       bgra = cv2.rectangle(bgra,(sx,sy),(sx+sw,sy+sh),(255,0,0),2)\n",
        "   else:\n",
        "       # 隨意在影像上繪製一個黃色實心方框\n",
        "       bgra = cv2.rectangle(bgra,(sx,sy),(sx+sw,sy+sh),(128,128,0),-1)\n",
        "\n",
        "   # 將有繪製圖案部份，透明層[3]設為完全不透明(255)\n",
        "   bgra[:,:,3] = (bgra.max(axis = 2) > 0 ).astype(int) * 255\n",
        "\n",
        "   return bgra"
      ],
      "metadata": {
        "id": "qxlA-mU5GadO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "sT5fV1c17Yls",
        "outputId": "6c5946d8-fbf8-4f83-ac3d-297550c617b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.48-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.13-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.48-py3-none-any.whl (898 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.8/898.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.13-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.48 ultralytics-thop-2.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "def realtime_process(image):\n",
        "\n",
        "  # # 读取图像\n",
        "  # image = cv2.imread('/content/5aad9ae03c0d2a49f99a8c616a8a7119.png')\n",
        "  # 加载模型\n",
        "  model = YOLO(\"yolo11n-seg.pt\")\n",
        "  # 预测\n",
        "  results = model(image)\n",
        "  # 创建空掩码\n",
        "  segmentation_mask = np.zeros_like(image, dtype=np.uint8)\n",
        "  # 迭代结果，生成人物掩码\n",
        "  for i, r in enumerate(results):\n",
        "      for j, mask in enumerate(r.masks.xy):\n",
        "          class_id = int(r.boxes.cls[j].item())\n",
        "          if class_id == 0:  # 人物类别ID为0\n",
        "              mask = np.array(mask, dtype=np.int32)\n",
        "              cv2.fillPoly(segmentation_mask, [mask], (255, 255, 255))\n",
        "\n",
        "  # # 假设segmentation_mask是单通道的灰度掩码\n",
        "  # bgra_mask = np.zeros_like(image, dtype=np.uint8)  # 创建一个空的BGRA图像\n",
        "  # bgra_mask[:, :, 0] = segmentation_mask  # 蓝色通道\n",
        "  # bgra_mask[:, :, 1] = segmentation_mask  # 绿色通道\n",
        "  # bgra_mask[:, :, 2] = segmentation_mask  # 红色通道\n",
        "  # bgra_mask[:, :, 3] = 255  # Alpha通道设置为不透明\n",
        "\n",
        "  # # 现在bgra_mask是一个BGRA格式的图像\n",
        "\n",
        "  return segmentation_mask"
      ],
      "metadata": {
        "id": "FmnTYxAj60a6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 将 OpenCV BGRA 格式影像转换成可以覆叠回串流影像之base64格式\n",
        "\n",
        "输入参数：  \n",
        "overlap_array : OpenCV BGRA 格式之输入影像\n",
        "\n",
        "回返结果:  \n",
        "overlap_bytes : PIL RGBA 转 base64 格式之输出影像"
      ],
      "metadata": {
        "id": "RRp4rhzf3aG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def overlap_to_bytes(overlap_array):\n",
        "  # 轉換 OpenCV BGRA 輸入格式到 PIL RGBA 格式\n",
        "  overlap_PIL = PIL.Image.fromarray(overlap_array)\n",
        "  # 建立一個 io 緩衝區\n",
        "  iobuf = io.BytesIO()\n",
        "  # 將轉換好的 PIL RGBA 格式影像存成帶透明圖層 PNG 格式到 io 緩衝區\n",
        "  overlap_PIL.save(iobuf, format='png')\n",
        "  # 轉換成 base64 格式\n",
        "  overlap_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return overlap_bytes"
      ],
      "metadata": {
        "id": "oHFw3H8d3a_U"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 启动网路摄影机连续取像并执行自定义影像处理程式\n",
        "\n",
        "执行过程中以滑鼠点击影像即会中止程式。  "
      ],
      "metadata": {
        "id": "nf_gva1R6ktN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 啟動網路攝影機開始接收串流影像\n",
        "video_stream()\n",
        "# 設定串流影像文字標籤\n",
        "label_html = 'Capturing...'\n",
        "# 清空疊合影像內容\n",
        "overlap_img = ''\n",
        "\n",
        "# 執行取像、處理循環，執行過程中若於影像上按下滑鼠鍵即會中止程式\n",
        "while True:\n",
        "    # 從網路攝影機取得串流影像目前影格\n",
        "    js_reply = video_frame(label_html, overlap_img)\n",
        "\n",
        "    # 若無法取得影格則結束循環\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # 將回傳的JavaScript影像物件轉成 OpenCV BGR 格式\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # 呼叫自定義影像即時處理函式\n",
        "    overlap_array = realtime_process(img)\n",
        "\n",
        "    # 將覆疊影像轉換成 base64 格式\n",
        "    overlap_bytes = overlap_to_bytes(overlap_array)\n",
        "\n",
        "    # 更新覆疊影像到下一個影格\n",
        "    overlap_img = overlap_bytes"
      ],
      "metadata": {
        "id": "dYv-VjOty2A0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "977c29eb-1429-4395-cc46-40192215b0f9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    \n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 480x640 1 person, 259.3ms\n",
            "Speed: 5.0ms preprocess, 259.3ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 253.6ms\n",
            "Speed: 3.3ms preprocess, 253.6ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 244.7ms\n",
            "Speed: 4.1ms preprocess, 244.7ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 435.8ms\n",
            "Speed: 3.8ms preprocess, 435.8ms inference, 8.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 255.2ms\n",
            "Speed: 3.5ms preprocess, 255.2ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 260.4ms\n",
            "Speed: 5.0ms preprocess, 260.4ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 270.8ms\n",
            "Speed: 5.0ms preprocess, 270.8ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 250.3ms\n",
            "Speed: 3.2ms preprocess, 250.3ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 250.5ms\n",
            "Speed: 3.5ms preprocess, 250.5ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 278.3ms\n",
            "Speed: 3.6ms preprocess, 278.3ms inference, 9.8ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-962908ef02b8>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# 從網路攝影機取得串流影像目前影格\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mjs_reply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 若無法取得影格則結束循環\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-5d75e0a649da>\u001b[0m in \u001b[0;36mvideo_frame\u001b[0;34m(label, bbox)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvideo_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stream_frame(\"{}\", \"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}